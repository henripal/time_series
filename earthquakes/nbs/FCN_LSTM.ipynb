{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast AI Time Series Competition No1: Earthquakes.\n",
    "\n",
    "Data is already downloaded in `../data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('../data')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earthquakes_TEST.txt',\n",
       " 'Earthquakes.txt',\n",
       " 'Earthquakes_TRAIN.arff',\n",
       " 'GADF',\n",
       " 'Earthquakes_TRAIN.txt',\n",
       " 'Earthquakes_TEST.arff']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt(data_path/'Earthquakes_TRAIN.txt')\n",
    "test = np.loadtxt(data_path/'Earthquakes_TEST.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = train[:, 1:], train[:, 0].astype(np.int)\n",
    "test, test_labels = test[:, 1:], test[:, 0].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchdata.TensorDataset(\n",
    "    torch.tensor(train, dtype=torch.float32),\n",
    "    torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1))\n",
    "test_ds = torchdata.TensorDataset(\n",
    "    torch.tensor(test, dtype=torch.float32),\n",
    "    torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torchdata.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = torchdata.DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fastai.DataBunch(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-FCNN\n",
    "This is a reimplementation of the LSTM-FCNN from https://ieeexplore-ieee-org.ezproxy.cul.columbia.edu/ielx7/6287639/8274985/08141873.pdf?tp=&arnumber=8141873&isnumber=8274985&tag=1\n",
    "\n",
    "The reported accuracy on the Earthquakes dataset in the paper is .8354."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleLSTM(nn.Module):\n",
    "    def __init__(self, seq_length, dropout=0.0, hidden_size=128, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=seq_length,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## input size will be the batch, input_size, seq_len =  64, 1, 512\n",
    "        ## we start with the shuffle layer, transforming this in a 1 step multivariate TS:\n",
    "        ## output should be 1, 64, 512\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        # input_size, batch, seq_len\n",
    "        out, (_, _) = self.lstm(x)\n",
    "        # after the lstm, the output will be 1, bs, lstm_size\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_channels=128, kernel_size=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels,\n",
    "                             out_channels=n_channels,\n",
    "                             kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(num_features=n_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input size should be batch, input_size, length = 64, 1, 312 at first\n",
    "        x = self.conv(x)\n",
    "        # now batch, n_channels, L\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        # output will be bs, n_filters, L (not equal to inital L)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_length,\n",
    "                 lstm_dropout=0.0,\n",
    "                 lstm_hidden_size=128,\n",
    "                 lstm_num_layers=1,\n",
    "                 lstm_bidirectional=False,\n",
    "                 conv_n_channels = [128, 256, 128],\n",
    "                 conv_kernel_sizes = [8, 5, 3],\n",
    "                 conv_dropout=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = ShuffleLSTM(seq_length,\n",
    "                                lstm_dropout,\n",
    "                                lstm_hidden_size,\n",
    "                                lstm_num_layers,\n",
    "                                lstm_bidirectional)\n",
    "        self.conv1 = ConvLayer(1, conv_n_channels[0], conv_kernel_sizes[0])  \n",
    "        self.conv2 = ConvLayer(conv_n_channels[0], conv_n_channels[1], conv_kernel_sizes[1])  \n",
    "        self.conv3 = ConvLayer(conv_n_channels[1], conv_n_channels[2], conv_kernel_sizes[2])  \n",
    "        \n",
    "        # global pool is avg pool using the length of the resulting TS\n",
    "        # we need to calculate this:\n",
    "        out_len = seq_length - sum(conv_kernel_sizes) + len(conv_kernel_sizes)\n",
    "        self.global_avg_pool = nn.AvgPool1d(kernel_size = out_len)\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(conv_n_channels[-1] + lstm_hidden_size, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # our input is batch, seq_len\n",
    "        x = x.unsqueeze(1)\n",
    "        # batch, 1, seq_len\n",
    "        \n",
    "        \n",
    "        lstm_out = self.lstm(x)\n",
    "        lstm_out = torch.squeeze(lstm_out)\n",
    "        # lstm_out is bs, lstm_size\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # x is now bs, dimension, L\n",
    "        # avgpool1d averages over the last dimension so we transpose again\n",
    "        x = self.global_avg_pool(x)\n",
    "        # bs, dimension, 1\n",
    "        x = torch.squeeze(x)\n",
    "        # bs, dimension\n",
    "        \n",
    "        concat = torch.cat((lstm_out, x), 1)\n",
    "        \n",
    "        return self.linear(concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_FCNN(512, lstm_dropout=0.8, lstm_hidden_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.551724137931035"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = len(train_labels)/ np.sum(train_labels) -1\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], dtype=torch.float32))\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "loss = loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_acc(pred, targ):\n",
    "    ok = torch.tensor(torch.abs(torch.sign(pred)/2 + .5 - targ) < .01, dtype=torch.float32)\n",
    "    return torch.mean(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = fastai.Learner(data, model, loss_func=loss, metrics=my_acc)\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find(); learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:28\n",
      "epoch  train_loss  valid_loss  my_acc  \n",
      "1      0.568187    0.672679    0.719424  (00:00)\n",
      "2      0.548042    0.666089    0.726619  (00:00)\n",
      "3      0.531417    0.649852    0.733813  (00:00)\n",
      "4      0.521316    0.627779    0.741007  (00:00)\n",
      "5      0.513305    0.600121    0.748201  (00:00)\n",
      "6      0.505123    0.575806    0.748201  (00:00)\n",
      "7      0.496105    0.559037    0.748201  (00:00)\n",
      "8      0.489058    0.546101    0.748201  (00:00)\n",
      "9      0.481128    0.535779    0.748201  (00:00)\n",
      "10     0.474845    0.528560    0.748201  (00:00)\n",
      "11     0.470002    0.525075    0.748201  (00:00)\n",
      "12     0.466731    0.522482    0.748201  (00:00)\n",
      "13     0.462355    0.520715    0.748201  (00:00)\n",
      "14     0.458115    0.518622    0.748201  (00:00)\n",
      "15     0.456863    0.518696    0.748201  (00:00)\n",
      "16     0.452798    0.518607    0.748201  (00:00)\n",
      "17     0.450655    0.519165    0.748201  (00:00)\n",
      "18     0.446936    0.576442    0.748201  (00:00)\n",
      "19     0.443479    0.530901    0.748201  (00:00)\n",
      "20     0.441210    0.519841    0.748201  (00:00)\n",
      "21     0.439044    0.566453    0.748201  (00:00)\n",
      "22     0.438525    0.631218    0.748201  (00:00)\n",
      "23     0.434893    0.534534    0.748201  (00:00)\n",
      "24     0.432486    0.529650    0.748201  (00:00)\n",
      "25     0.430854    0.569515    0.748201  (00:00)\n",
      "26     0.427882    0.528029    0.748201  (00:00)\n",
      "27     0.424905    0.638329    0.748201  (00:00)\n",
      "28     0.421852    0.602597    0.748201  (00:00)\n",
      "29     0.419048    0.599697    0.748201  (00:00)\n",
      "30     0.416804    0.516732    0.748201  (00:00)\n",
      "31     0.415615    0.595768    0.748201  (00:00)\n",
      "32     0.413698    1.662078    0.748201  (00:00)\n",
      "33     0.412797    2.234053    0.748201  (00:00)\n",
      "34     0.411628    2.124601    0.748201  (00:00)\n",
      "35     0.410058    1.596587    0.748201  (00:00)\n",
      "36     0.407261    0.903837    0.748201  (00:00)\n",
      "37     0.404138    0.552137    0.755396  (00:00)\n",
      "38     0.401092    0.737663    0.395683  (00:00)\n",
      "39     0.397731    0.565091    0.748201  (00:00)\n",
      "40     0.394944    0.539673    0.755396  (00:00)\n",
      "41     0.392280    0.532886    0.755396  (00:00)\n",
      "42     0.388445    0.603684    0.748201  (00:00)\n",
      "43     0.385161    0.581216    0.748201  (00:00)\n",
      "44     0.381868    0.538930    0.748201  (00:00)\n",
      "45     0.378565    0.522605    0.755396  (00:00)\n",
      "46     0.377344    0.557513    0.755396  (00:00)\n",
      "47     0.374394    0.574999    0.690647  (00:00)\n",
      "48     0.370651    0.654589    0.748201  (00:00)\n",
      "49     0.368014    0.728922    0.748201  (00:00)\n",
      "50     0.366004    0.678701    0.748201  (00:00)\n",
      "51     0.363772    0.559083    0.748201  (00:00)\n",
      "52     0.360427    0.810480    0.410072  (00:00)\n",
      "53     0.358179    0.532609    0.755396  (00:00)\n",
      "54     0.355226    0.538269    0.748201  (00:00)\n",
      "55     0.352043    0.596793    0.748201  (00:00)\n",
      "56     0.349674    0.520058    0.748201  (00:00)\n",
      "57     0.346016    0.560175    0.748201  (00:00)\n",
      "58     0.342492    0.643519    0.582734  (00:00)\n",
      "59     0.339471    0.638307    0.748201  (00:00)\n",
      "60     0.335371    0.602022    0.748201  (00:00)\n",
      "61     0.331272    1.152905    0.748201  (00:00)\n",
      "62     0.329042    0.659567    0.748201  (00:00)\n",
      "63     0.325755    0.537837    0.697842  (00:00)\n",
      "64     0.323379    0.604503    0.625899  (00:00)\n",
      "65     0.319531    0.927967    0.366906  (00:00)\n",
      "66     0.316731    0.657631    0.748201  (00:00)\n",
      "67     0.313628    1.318656    0.748201  (00:00)\n",
      "68     0.310549    1.513395    0.748201  (00:00)\n",
      "69     0.307505    0.567877    0.748201  (00:00)\n",
      "70     0.303531    0.673916    0.589928  (00:00)\n",
      "71     0.298964    0.583380    0.697842  (00:00)\n",
      "72     0.294592    0.543971    0.748201  (00:00)\n",
      "73     0.290238    2.484695    0.748201  (00:00)\n",
      "74     0.287908    2.097119    0.748201  (00:00)\n",
      "75     0.285529    0.930939    0.748201  (00:00)\n",
      "76     0.284022    1.903802    0.266187  (00:00)\n",
      "77     0.281488    0.663536    0.597122  (00:00)\n",
      "78     0.278502    0.580307    0.748201  (00:00)\n",
      "79     0.275068    1.199006    0.748201  (00:00)\n",
      "80     0.270866    1.004282    0.748201  (00:00)\n",
      "81     0.266735    0.908657    0.438849  (00:00)\n",
      "82     0.262737    0.540064    0.697842  (00:00)\n",
      "83     0.259267    0.707205    0.748201  (00:00)\n",
      "84     0.256285    2.072744    0.748201  (00:00)\n",
      "85     0.252569    1.946329    0.748201  (00:00)\n",
      "86     0.250006    1.484421    0.748201  (00:00)\n",
      "87     0.247417    0.745535    0.517986  (00:00)\n",
      "88     0.243395    0.540485    0.690647  (00:00)\n",
      "89     0.239928    1.196008    0.359712  (00:00)\n",
      "90     0.235355    0.553134    0.748201  (00:00)\n",
      "91     0.232781    0.875271    0.748201  (00:00)\n",
      "92     0.229165    1.563813    0.748201  (00:00)\n",
      "93     0.225238    0.568092    0.719424  (00:00)\n",
      "94     0.221413    1.262981    0.748201  (00:00)\n",
      "95     0.218431    0.602758    0.618705  (00:00)\n",
      "96     0.216660    1.025609    0.748201  (00:00)\n",
      "97     0.215650    1.016625    0.438849  (00:00)\n",
      "98     0.212699    0.528367    0.719424  (00:00)\n",
      "99     0.209137    1.321465    0.748201  (00:00)\n",
      "100    0.205813    1.338978    0.748201  (00:00)\n",
      "101    0.201831    1.184745    0.748201  (00:00)\n",
      "102    0.198691    0.546975    0.683453  (00:00)\n",
      "103    0.195037    1.398678    0.374101  (00:00)\n",
      "104    0.191055    0.875892    0.748201  (00:00)\n",
      "105    0.188427    1.305359    0.748201  (00:00)\n",
      "106    0.185604    1.553218    0.748201  (00:00)\n",
      "107    0.181712    3.314462    0.748201  (00:00)\n",
      "108    0.179528    1.184709    0.474820  (00:00)\n",
      "109    0.176965    6.022554    0.251799  (00:00)\n",
      "110    0.176607    1.521157    0.431655  (00:00)\n",
      "111    0.174492    0.530261    0.719424  (00:00)\n",
      "112    0.171924    2.081202    0.748201  (00:00)\n",
      "113    0.169930    1.938673    0.748201  (00:00)\n",
      "114    0.166635    4.201212    0.748201  (00:00)\n",
      "115    0.165111    0.669636    0.748201  (00:00)\n",
      "116    0.162785    0.623441    0.661870  (00:00)\n",
      "117    0.161608    2.236314    0.294964  (00:00)\n",
      "118    0.158929    0.681457    0.748201  (00:00)\n",
      "119    0.156732    0.843182    0.748201  (00:00)\n",
      "120    0.153516    0.717395    0.748201  (00:00)\n",
      "121    0.150666    2.758334    0.748201  (00:00)\n",
      "122    0.147748    0.812960    0.741007  (00:00)\n",
      "123    0.145005    0.641862    0.741007  (00:00)\n",
      "124    0.142956    0.622369    0.647482  (00:00)\n",
      "125    0.139655    0.576089    0.733813  (00:00)\n",
      "126    0.136769    0.566628    0.726619  (00:00)\n",
      "127    0.133855    1.270119    0.748201  (00:00)\n",
      "128    0.131299    0.580132    0.676259  (00:00)\n",
      "129    0.129000    0.722755    0.748201  (00:00)\n",
      "130    0.126868    1.140435    0.748201  (00:00)\n",
      "131    0.124689    1.131257    0.748201  (00:00)\n",
      "132    0.122037    1.882122    0.748201  (00:00)\n",
      "133    0.119486    1.847794    0.748201  (00:00)\n",
      "134    0.117500    1.084434    0.381295  (00:00)\n",
      "135    0.115727    0.579973    0.712230  (00:00)\n",
      "136    0.113044    1.390164    0.402878  (00:00)\n",
      "137    0.111656    3.077448    0.748201  (00:00)\n",
      "138    0.108994    1.378512    0.748201  (00:00)\n",
      "139    0.107171    3.109840    0.748201  (00:00)\n",
      "140    0.105337    0.616166    0.712230  (00:00)\n",
      "141    0.103642    1.756767    0.374101  (00:00)\n",
      "142    0.101730    2.806247    0.302158  (00:00)\n",
      "143    0.099327    1.010470    0.748201  (00:00)\n",
      "144    0.096924    0.812274    0.741007  (00:00)\n",
      "145    0.094285    1.153486    0.748201  (00:00)\n",
      "146    0.091804    1.040946    0.748201  (00:00)\n",
      "147    0.089590    1.804646    0.748201  (00:00)\n",
      "148    0.087303    0.584741    0.719424  (00:00)\n",
      "149    0.085125    0.726115    0.611511  (00:00)\n",
      "150    0.083514    0.607989    0.719424  (00:00)\n",
      "151    0.081376    0.619255    0.697842  (00:00)\n",
      "152    0.079770    0.958637    0.748201  (00:00)\n",
      "153    0.077799    3.394064    0.748201  (00:00)\n",
      "154    0.076913    3.791367    0.251799  (00:00)\n",
      "155    0.075743    2.194506    0.366906  (00:00)\n",
      "156    0.074904    3.217301    0.748201  (00:00)\n",
      "157    0.074446    2.532502    0.748201  (00:00)\n",
      "158    0.073746    5.887239    0.748201  (00:00)\n",
      "159    0.073655    0.634690    0.762590  (00:00)\n",
      "160    0.073586    0.591854    0.741007  (00:00)\n",
      "161    0.073845    1.534968    0.748201  (00:00)\n",
      "162    0.073136    0.812620    0.517986  (00:00)\n",
      "163    0.072586    0.644000    0.748201  (00:00)\n",
      "164    0.071438    0.709417    0.741007  (00:00)\n",
      "165    0.070475    2.201645    0.309353  (00:00)\n",
      "166    0.069191    0.695339    0.733813  (00:00)\n",
      "167    0.068260    2.743840    0.748201  (00:00)\n",
      "168    0.068217    0.975757    0.741007  (00:00)\n",
      "169    0.066362    0.827433    0.733813  (00:00)\n",
      "170    0.065071    0.645223    0.748201  (00:00)\n",
      "171    0.063059    1.336612    0.748201  (00:00)\n",
      "172    0.061602    1.770662    0.748201  (00:00)\n",
      "173    0.059674    0.607786    0.640288  (00:00)\n",
      "174    0.058094    0.710248    0.733813  (00:00)\n",
      "175    0.056547    1.148508    0.424460  (00:00)\n",
      "176    0.055248    0.742543    0.618705  (00:00)\n",
      "177    0.053657    2.907306    0.748201  (00:00)\n",
      "178    0.052905    0.655252    0.683453  (00:00)\n",
      "179    0.052847    2.970961    0.748201  (00:00)\n",
      "180    0.053872    8.378134    0.251799  (00:00)\n",
      "181    0.054410    1.743340    0.748201  (00:00)\n",
      "182    0.053670    6.259841    0.748201  (00:00)\n",
      "183    0.054121    0.799473    0.741007  (00:00)\n",
      "184    0.054013    2.500675    0.748201  (00:00)\n",
      "185    0.053462    4.260457    0.748201  (00:00)\n",
      "186    0.053894    0.674670    0.625899  (00:00)\n",
      "187    0.052875    0.754357    0.546763  (00:00)\n",
      "188    0.051745    0.928387    0.741007  (00:00)\n",
      "189    0.050601    1.343609    0.453237  (00:00)\n",
      "190    0.049759    6.285894    0.251799  (00:00)\n",
      "191    0.048563    0.694113    0.748201  (00:00)\n",
      "192    0.047441    2.091032    0.748201  (00:00)\n",
      "193    0.046047    3.606059    0.748201  (00:00)\n",
      "194    0.045289    5.218515    0.748201  (00:00)\n",
      "195    0.044978    0.653801    0.683453  (00:00)\n",
      "196    0.044306    2.599667    0.338129  (00:00)\n",
      "197    0.043271    0.931690    0.741007  (00:00)\n",
      "198    0.042558    1.598892    0.453237  (00:00)\n",
      "199    0.041559    1.698397    0.467626  (00:00)\n",
      "200    0.040591    1.199306    0.741007  (00:00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(200, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
