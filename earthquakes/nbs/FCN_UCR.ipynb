{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast AI Time Series Competition No1: Earthquakes.\n",
    "\n",
    "Data is already downloaded in `../data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('../data')\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earthquakes_TEST.txt',\n",
       " 'Earthquakes.txt',\n",
       " 'Earthquakes_TRAIN.arff',\n",
       " 'GADF',\n",
       " 'Earthquakes_TRAIN.txt',\n",
       " 'Earthquakes_TEST.arff']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt(data_path/'Earthquakes_TRAIN.txt')\n",
    "test = np.loadtxt(data_path/'Earthquakes_TEST.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = train[:, 1:], train[:, 0].astype(np.int)\n",
    "test, test_labels = test[:, 1:], test[:, 0].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchdata.TensorDataset(\n",
    "    torch.tensor(train, dtype=torch.float32),\n",
    "    torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1))\n",
    "test_ds = torchdata.TensorDataset(\n",
    "    torch.tensor(test, dtype=torch.float32),\n",
    "    torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torchdata.DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "test_dl = torchdata.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fastai.DataBunch(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-FCNN\n",
    "This is a reimplementation of the LSTM-FCNN from https://ieeexplore-ieee-org.ezproxy.cul.columbia.edu/ielx7/6287639/8274985/08141873.pdf?tp=&arnumber=8141873&isnumber=8274985&tag=1\n",
    "\n",
    "The reported accuracy on the Earthquakes dataset in the paper is .8354."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleLSTM(nn.Module):\n",
    "    def __init__(self, seq_length, dropout=0.0, hidden_size=128, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=seq_length,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## input size will be the batch, input_size, seq_len =  64, 1, 512\n",
    "        ## we start with the shuffle layer, transforming this in a 1 step multivariate TS:\n",
    "        ## output should be 1, 64, 512\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        # input_size, batch, seq_len\n",
    "        out, (_, _) = self.lstm(x)\n",
    "        # after the lstm, the output will be 1, bs, lstm_size\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_channels=128, kernel_size=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels,\n",
    "                             out_channels=n_channels,\n",
    "                             kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(num_features=n_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input size should be batch, input_size, length = 64, 1, 312 at first\n",
    "        x = self.conv(x)\n",
    "        # now batch, n_channels, L\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        # output will be bs, n_filters, L (not equal to inital L)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_length,\n",
    "                 lstm_dropout=0.0,\n",
    "                 lstm_hidden_size=128,\n",
    "                 lstm_num_layers=1,\n",
    "                 lstm_bidirectional=False,\n",
    "                 conv_n_channels = [128, 256, 128],\n",
    "                 conv_kernel_sizes = [8, 5, 3],\n",
    "                 conv_dropout=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = ShuffleLSTM(seq_length,\n",
    "                                lstm_dropout,\n",
    "                                lstm_hidden_size,\n",
    "                                lstm_num_layers,\n",
    "                                lstm_bidirectional)\n",
    "        self.conv1 = ConvLayer(1, conv_n_channels[0], conv_kernel_sizes[0])  \n",
    "        self.conv2 = ConvLayer(conv_n_channels[0], conv_n_channels[1], conv_kernel_sizes[1])  \n",
    "        self.conv3 = ConvLayer(conv_n_channels[1], conv_n_channels[2], conv_kernel_sizes[2])  \n",
    "        \n",
    "        # global pool is avg pool using the length of the resulting TS\n",
    "        # we need to calculate this:\n",
    "        out_len = seq_length - sum(conv_kernel_sizes) + len(conv_kernel_sizes)\n",
    "        self.global_avg_pool = nn.AvgPool1d(kernel_size = out_len)\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(conv_n_channels[-1] + lstm_hidden_size, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # our input is batch, seq_len\n",
    "        x = x.unsqueeze(1)\n",
    "        # batch, 1, seq_len\n",
    "        \n",
    "        \n",
    "        lstm_out = self.lstm(x)\n",
    "        lstm_out = torch.squeeze(lstm_out)\n",
    "        # lstm_out is bs, lstm_size\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # x is now bs, dimension, L\n",
    "        # avgpool1d averages over the last dimension so we transpose again\n",
    "        x = self.global_avg_pool(x)\n",
    "        # bs, dimension, 1\n",
    "        x = torch.squeeze(x)\n",
    "        # bs, dimension\n",
    "        \n",
    "        concat = torch.cat((lstm_out, x), 1)\n",
    "        \n",
    "        return self.linear(concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_FCNN(512, lstm_dropout=0.8, lstm_hidden_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.551724137931035"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = len(train_labels)/ np.sum(train_labels) -1\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], dtype=torch.float32))\n",
    "#loss = nn.BCEWithLogitsLoss()\n",
    "loss = loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_acc(pred, targ):\n",
    "    ok = torch.tensor(torch.abs(torch.sign(pred)/2 + .5 - targ) < .01, dtype=torch.float32)\n",
    "    return torch.mean(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = fastai.Learner(data, model, loss_func=loss, metrics=my_acc)\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find(); learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:20\n",
      "epoch  train_loss  valid_loss  my_acc  \n",
      "1      1.027947    1.296351    0.460432  (00:00)\n",
      "2      1.007556    1.223641    0.568345  (00:00)\n",
      "3      0.995456    1.184080    0.561151  (00:00)\n",
      "4      0.985683    1.150312    0.517986  (00:00)\n",
      "5      0.978588    1.152022    0.525180  (00:00)\n",
      "6      0.970651    1.144507    0.517986  (00:00)\n",
      "7      0.965992    1.139562    0.517986  (00:00)\n",
      "8      0.959701    1.170578    0.568345  (00:00)\n",
      "9      0.951855    1.143247    0.525180  (00:00)\n",
      "10     0.948136    1.157148    0.546763  (00:00)\n",
      "11     0.940357    1.153579    0.546763  (00:00)\n",
      "12     0.932276    1.677880    0.748201  (00:00)\n",
      "13     0.920552    1.146090    0.482014  (00:00)\n",
      "14     0.912557    1.394829    0.697842  (00:00)\n",
      "15     0.903140    2.431006    0.748201  (00:00)\n",
      "16     0.898813    1.919390    0.294964  (00:00)\n",
      "17     0.894403    4.207318    0.748201  (00:00)\n",
      "18     0.886037    1.219529    0.633094  (00:00)\n",
      "19     0.870746    1.198161    0.575540  (00:00)\n",
      "20     0.859127    1.191256    0.553957  (00:00)\n",
      "21     0.843637    1.625608    0.755396  (00:00)\n",
      "22     0.824850    2.352509    0.748201  (00:00)\n",
      "23     0.803759    1.382080    0.683453  (00:00)\n",
      "24     0.785256    2.010905    0.748201  (00:00)\n",
      "25     0.768252    1.172868    0.467626  (00:00)\n",
      "26     0.760770    2.337371    0.748201  (00:00)\n",
      "27     0.748562    1.442224    0.741007  (00:00)\n",
      "28     0.734996    3.754886    0.748201  (00:00)\n",
      "29     0.725715    1.414055    0.726619  (00:00)\n",
      "30     0.711149    1.303239    0.654676  (00:00)\n",
      "31     0.688356    2.396363    0.748201  (00:00)\n",
      "32     0.664597    2.015334    0.748201  (00:00)\n",
      "33     0.648562    1.184276    0.517986  (00:00)\n",
      "34     0.633304    4.935092    0.748201  (00:00)\n",
      "35     0.621736    1.318228    0.647482  (00:00)\n",
      "36     0.603297    1.684252    0.733813  (00:00)\n",
      "37     0.586923    1.916315    0.741007  (00:00)\n",
      "38     0.580428    1.403588    0.705036  (00:00)\n",
      "39     0.575548    8.369519    0.748201  (00:00)\n",
      "40     0.566402    1.256134    0.467626  (00:00)\n",
      "41     0.552873    1.224413    0.604317  (00:00)\n",
      "42     0.537434    1.988412    0.755396  (00:00)\n",
      "43     0.518576    1.393000    0.676259  (00:00)\n",
      "44     0.500691    5.553904    0.748201  (00:00)\n",
      "45     0.472426    1.304156    0.640288  (00:00)\n",
      "46     0.450896    1.281785    0.424460  (00:00)\n",
      "47     0.427776    3.336737    0.748201  (00:00)\n",
      "48     0.408435    1.398546    0.366906  (00:00)\n",
      "49     0.394482    2.560863    0.741007  (00:00)\n",
      "50     0.385357    4.765849    0.748201  (00:00)\n",
      "51     0.369944    1.640114    0.338129  (00:00)\n",
      "52     0.358534    3.810718    0.748201  (00:00)\n",
      "53     0.352376    1.635213    0.431655  (00:00)\n",
      "54     0.340940    1.443969    0.676259  (00:00)\n",
      "55     0.328885    12.067010   0.748201  (00:00)\n",
      "56     0.322081    3.222528    0.741007  (00:00)\n",
      "57     0.312268    4.301001    0.258993  (00:00)\n",
      "58     0.300106    1.563963    0.661870  (00:00)\n",
      "59     0.292316    1.646343    0.388489  (00:00)\n",
      "60     0.277829    10.325224   0.748201  (00:00)\n",
      "61     0.281491    1.463459    0.438849  (00:00)\n",
      "62     0.274138    4.975184    0.748201  (00:00)\n",
      "63     0.274036    11.153468   0.748201  (00:00)\n",
      "64     0.276743    10.647917   0.748201  (00:00)\n",
      "65     0.273789    6.669998    0.251799  (00:00)\n",
      "66     0.264369    8.966958    0.748201  (00:00)\n",
      "67     0.256116    6.065012    0.748201  (00:00)\n",
      "68     0.244473    3.200993    0.748201  (00:00)\n",
      "69     0.227288    4.578192    0.748201  (00:00)\n",
      "70     0.208352    2.823735    0.741007  (00:00)\n",
      "71     0.190867    5.736117    0.748201  (00:00)\n",
      "72     0.176979    5.687983    0.748201  (00:00)\n",
      "73     0.175469    4.105185    0.748201  (00:00)\n",
      "74     0.172316    1.584564    0.352518  (00:00)\n",
      "75     0.160326    3.089727    0.273381  (00:00)\n",
      "76     0.155443    2.195300    0.726619  (00:00)\n",
      "77     0.145268    2.136622    0.719424  (00:00)\n",
      "78     0.137340    8.300595    0.748201  (00:00)\n",
      "79     0.146268    5.738967    0.755396  (00:00)\n",
      "80     0.159537    23.450468   0.748201  (00:00)\n",
      "81     0.191194    2.413914    0.712230  (00:00)\n",
      "82     0.202394    2.056108    0.748201  (00:00)\n",
      "83     0.195890    3.638179    0.266187  (00:00)\n",
      "84     0.186078    2.088288    0.719424  (00:00)\n",
      "85     0.169720    2.525315    0.733813  (00:00)\n",
      "86     0.153126    4.496409    0.741007  (00:00)\n",
      "87     0.137042    2.350025    0.733813  (00:00)\n",
      "88     0.122622    1.672785    0.589928  (00:00)\n",
      "89     0.116441    5.521941    0.748201  (00:00)\n",
      "90     0.109147    3.027448    0.402878  (00:00)\n",
      "91     0.104212    10.656910   0.748201  (00:00)\n",
      "92     0.095306    11.283701   0.748201  (00:00)\n",
      "93     0.087561    1.656637    0.553957  (00:00)\n",
      "94     0.085038    8.512464    0.748201  (00:00)\n",
      "95     0.088633    1.723414    0.532374  (00:00)\n",
      "96     0.082922    9.684493    0.748201  (00:00)\n",
      "97     0.077689    3.078942    0.395683  (00:00)\n",
      "98     0.086901    2.063504    0.482014  (00:00)\n",
      "99     0.097572    1.914944    0.474820  (00:00)\n",
      "100    0.100588    7.712119    0.748201  (00:00)\n",
      "101    0.112515    7.320925    0.748201  (00:00)\n",
      "102    0.113478    17.252762   0.748201  (00:00)\n",
      "103    0.111162    22.205587   0.748201  (00:00)\n",
      "104    0.110069    1.572545    0.482014  (00:00)\n",
      "105    0.106006    1.542561    0.525180  (00:00)\n",
      "106    0.096589    2.153030    0.417266  (00:00)\n",
      "107    0.089747    22.899216   0.748201  (00:00)\n",
      "108    0.084862    2.174695    0.446043  (00:00)\n",
      "109    0.080479    5.108158    0.741007  (00:00)\n",
      "110    0.077861    32.277405   0.748201  (00:00)\n",
      "111    0.074160    15.013778   0.748201  (00:00)\n",
      "112    0.091813    9.591700    0.251799  (00:00)\n",
      "113    0.092104    9.807841    0.748201  (00:00)\n",
      "114    0.086974    3.393355    0.762590  (00:00)\n",
      "115    0.080946    14.491385   0.748201  (00:00)\n",
      "116    0.076189    15.126472   0.748201  (00:00)\n",
      "117    0.070597    2.530238    0.676259  (00:00)\n",
      "118    0.065624    3.112048    0.719424  (00:00)\n",
      "119    0.060386    2.091241    0.669065  (00:00)\n",
      "120    0.058062    13.110382   0.748201  (00:00)\n",
      "121    0.053990    14.439296   0.748201  (00:00)\n",
      "122    0.051102    2.192038    0.366906  (00:00)\n",
      "123    0.047173    1.759840    0.496403  (00:00)\n",
      "124    0.044743    5.397119    0.733813  (00:00)\n",
      "125    0.041567    17.379139   0.748201  (00:00)\n",
      "126    0.040552    3.229999    0.741007  (00:00)\n",
      "127    0.037795    5.153181    0.748201  (00:00)\n",
      "128    0.042732    21.691011   0.748201  (00:00)\n",
      "129    0.045475    3.566049    0.388489  (00:00)\n",
      "130    0.047192    2.564376    0.410072  (00:00)\n",
      "131    0.045487    2.050783    0.417266  (00:00)\n",
      "132    0.043911    6.252448    0.748201  (00:00)\n",
      "133    0.039742    1.861237    0.446043  (00:00)\n",
      "134    0.038709    13.752970   0.748201  (00:00)\n",
      "135    0.039234    11.335556   0.251799  (00:00)\n",
      "136    0.051977    25.965506   0.748201  (00:00)\n",
      "137    0.067988    19.486168   0.748201  (00:00)\n",
      "138    0.069136    8.703411    0.251799  (00:00)\n",
      "139    0.068953    2.929699    0.352518  (00:00)\n",
      "140    0.078298    1.966032    0.669065  (00:00)\n",
      "141    0.077145    5.809247    0.741007  (00:00)\n",
      "142    0.076441    13.574749   0.748201  (00:00)\n",
      "143    0.074398    18.930630   0.748201  (00:00)\n",
      "144    0.067738    2.390892    0.345324  (00:00)\n",
      "145    0.060216    7.360979    0.748201  (00:00)\n",
      "146    0.052041    2.817842    0.683453  (00:00)\n",
      "147    0.045319    5.779915    0.748201  (00:00)\n",
      "148    0.039895    4.464010    0.359712  (00:00)\n",
      "149    0.034682    2.698817    0.676259  (00:00)\n",
      "150    0.030087    4.117457    0.726619  (00:00)\n",
      "151    0.026820    10.262405   0.748201  (00:00)\n",
      "152    0.023865    2.680807    0.446043  (00:00)\n",
      "153    0.023123    29.247622   0.748201  (00:00)\n",
      "154    0.035296    7.307407    0.762590  (00:00)\n",
      "155    0.067095    11.418816   0.251799  (00:00)\n",
      "156    0.078915    12.881405   0.748201  (00:00)\n",
      "157    0.088366    2.618785    0.676259  (00:00)\n",
      "158    0.083725    3.351954    0.741007  (00:00)\n",
      "159    0.074575    3.306199    0.726619  (00:00)\n",
      "160    0.065936    1.783875    0.517986  (00:00)\n",
      "161    0.058211    3.676404    0.726619  (00:00)\n",
      "162    0.049739    10.314469   0.748201  (00:00)\n",
      "163    0.049455    36.073700   0.748201  (00:00)\n",
      "164    0.048899    2.293575    0.604317  (00:00)\n",
      "165    0.045280    1.925600    0.517986  (00:00)\n",
      "166    0.043283    2.943525    0.690647  (00:00)\n",
      "167    0.039775    14.316853   0.748201  (00:00)\n",
      "168    0.035187    2.185725    0.496403  (00:00)\n",
      "169    0.031386    2.573866    0.647482  (00:00)\n",
      "170    0.027304    4.132403    0.726619  (00:00)\n",
      "171    0.023416    3.594176    0.690647  (00:00)\n",
      "172    0.021727    14.819294   0.748201  (00:00)\n",
      "173    0.019573    3.466521    0.705036  (00:00)\n",
      "174    0.017355    8.816840    0.748201  (00:00)\n",
      "175    0.017099    5.422277    0.381295  (00:00)\n",
      "176    0.015970    2.554400    0.647482  (00:00)\n",
      "177    0.014847    2.171686    0.467626  (00:00)\n",
      "178    0.013444    3.136624    0.683453  (00:00)\n",
      "179    0.012297    9.301269    0.748201  (00:00)\n",
      "180    0.011305    3.349991    0.690647  (00:00)\n",
      "181    0.012296    4.837768    0.733813  (00:00)\n",
      "182    0.012048    5.597977    0.755396  (00:00)\n",
      "183    0.014158    2.581210    0.381295  (00:00)\n",
      "184    0.018917    29.939976   0.748201  (00:00)\n",
      "185    0.054706    12.823071   0.251799  (00:00)\n",
      "186    0.121315    23.525862   0.748201  (00:00)\n",
      "187    0.136275    22.393847   0.748201  (00:00)\n",
      "188    0.139564    2.269348    0.388489  (00:00)\n",
      "189    0.135127    8.408726    0.251799  (00:00)\n",
      "190    0.125076    8.044933    0.748201  (00:00)\n",
      "191    0.111078    1.985573    0.402878  (00:00)\n",
      "192    0.097093    12.570008   0.748201  (00:00)\n",
      "193    0.087552    19.652826   0.748201  (00:00)\n",
      "194    0.079522    4.221992    0.309353  (00:00)\n",
      "195    0.069413    7.017453    0.266187  (00:00)\n",
      "196    0.060086    12.265329   0.748201  (00:00)\n",
      "197    0.051642    13.017400   0.748201  (00:00)\n",
      "198    0.043980    11.875920   0.748201  (00:00)\n",
      "199    0.037150    3.685694    0.719424  (00:00)\n",
      "200    0.032263    4.533674    0.719424  (00:00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(200, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_BAD_PARAM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-967f8f5b74c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3b02a4c0cbba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# lstm_out is bs, lstm_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d99e5b282c45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# input_size, batch, seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# after the lstm, the output will be 1, bs, lstm_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_BAD_PARAM"
     ]
    }
   ],
   "source": [
    "learner.model(torch.tensor(train).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(test, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_FCNN(\n",
       "  (lstm): ShuffleLSTM(\n",
       "    (lstm): LSTM(512, 32)\n",
       "    (dropout): Dropout(p=0.8)\n",
       "  )\n",
       "  (conv1): ConvLayer(\n",
       "    (conv): Conv1d(1, 128, kernel_size=(8,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (conv2): ConvLayer(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (conv3): ConvLayer(\n",
       "    (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (global_avg_pool): AvgPool1d(kernel_size=(499,), stride=(499,), padding=(0,))\n",
       "  (linear): Linear(in_features=160, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = (np.sign(out)/2 + .5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_labels == out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data.valid_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_FCNN(\n",
       "  (lstm): ShuffleLSTM(\n",
       "    (lstm): LSTM(512, 32)\n",
       "    (dropout): Dropout(p=0.8)\n",
       "  )\n",
       "  (conv1): ConvLayer(\n",
       "    (conv): Conv1d(1, 128, kernel_size=(8,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (conv2): ConvLayer(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (conv3): ConvLayer(\n",
       "    (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (global_avg_pool): AvgPool1d(kernel_size=(499,), stride=(499,), padding=(0,))\n",
       "  (linear): Linear(in_features=160, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "LSTM(512, 32)\n",
      "True\n",
      "Dropout(p=0.8)\n",
      "True\n",
      "True\n",
      "Conv1d(1, 128, kernel_size=(8,), stride=(1,))\n",
      "True\n",
      "BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "True\n",
      "Dropout(p=0.0)\n",
      "True\n",
      "True\n",
      "Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
      "True\n",
      "BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "True\n",
      "Dropout(p=0.0)\n",
      "True\n",
      "True\n",
      "Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      "True\n",
      "BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "True\n",
      "Dropout(p=0.0)\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for m in model.children():\n",
    "    print(m.training)\n",
    "    for j in m.children():\n",
    "        print(j)\n",
    "        print(j.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198757763975155"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
